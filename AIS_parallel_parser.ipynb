{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7581cd75-ef9f-4587-9bce-49cd81f5ca25",
   "metadata": {},
   "source": [
    "### A notebook to test parallelization of the `Ais` class in `NPS-ActiveSpace.utils.models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e041665-25aa-46bf-82cc-273dbf75910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import cmocean as cm\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "from scipy import stats\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# general NSNSD acoustical tools\n",
    "# =======================================================================================================\n",
    "repo_dir = r\"C:\\Users\\DBetchkal\\PythonScripts\\GITHUB\" # ADJUST TO YOUR LOCAL DIRECTORY\n",
    "# =======================================================================================================\n",
    "sys.path.append(os.path.join(repo_dir, \"iyore\"))\n",
    "sys.path.append(os.path.join(repo_dir, \"NPS-ActiveSpace\"))\n",
    "\n",
    "import iyore\n",
    "import nps_active_space\n",
    "from nps_active_space.utils.models import Ais\n",
    "\n",
    "def NVSPL_to_AIS_subset(acoustic_ds, vessel_ds, unit, site):\n",
    "    \"\"\"\n",
    "    Use the dates of available acoustic data (as NVSPL files)\n",
    "    to reference the paths of corresponding transportation data (as raw AIS files).\n",
    "\n",
    "    This allows the analyst to parse only the relevant data.\n",
    "    \"\"\"\n",
    "    \n",
    "    AIS_to_parse = []\n",
    "    for entry in tqdm(acoustic_ds.nvspl(unit=unit, site=site), unit=\"NVSPL files\"):\n",
    "        paths = [e.path for e in vessel_ds.AIS(year=int(entry.year), month=int(entry.month), day=int(entry.day))] # WILL BE OFF BY +9 HOURS! ðŸ˜§\n",
    "        for path in paths:\n",
    "            AIS_to_parse.append(path) \n",
    "    AIS_to_parse = np.unique(AIS_to_parse)\n",
    "\n",
    "    return AIS_to_parse\n",
    "\n",
    "# import pytz\n",
    "# set(pytz.all_timezones_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce803560-9736-4ad6-9989-2313d912f00c",
   "metadata": {},
   "source": [
    "### Step 1: compile (raw) AIS data filepaths into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defa36ce-1a5f-4def-b3d3-71663b582e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# where are the AIS data stored? (with the requisite `iyore` .structure.txt file)\n",
    "# csv_path = r\"C:\\Users\\DBetchkal\\Documents\\ArcGIS\\Projects\\GLBA_AIS_2022\\MXAK-AIS-GLBA\"\n",
    "csv_path = r\"V:\\Noncanonical Data\\2024 - 2020 MXAK-AIS-GLBA\\MXAK-AIS-GLBA\"\n",
    "ds_AIS = iyore.Dataset(csv_path) # initialize an `iyore` dataset object\n",
    "\n",
    "ds = iyore.Dataset(r\"E:\\Sound Data\") # where are the acoustic data stored?\n",
    "# iyore_subset = NVSPL_to_AIS_subset(ds, ds_AIS, \"GLBA\", \"RENDU\") # this function finds every AIS file which corresponds to an acoustic record (a bit slow)\n",
    "# print(\"OK, we found\", len(iyore_subset), \"relevant AIS dates to parse\")\n",
    "\n",
    "# or you can just work with every single AIS file\n",
    "# iyore_subset = glob.glob(r\"C:\\Users\\DBetchkal\\Documents\\ArcGIS\\Projects\\GLBA_AIS_2022\\MXAK-AIS-GLBA\\MXAK-AIS-GLBA-*.csv\")\n",
    "iyore_subset = glob.glob(csv_path+os.sep+\"*.csv\")\n",
    "\n",
    "iyore_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6454fdf-6d04-42f4-9098-2eff3a1a79e1",
   "metadata": {},
   "source": [
    "### Step 2: parse the entire AIS dataset\n",
    "#### Without parallelization the parsing rate was 4.7 seconds per file (02:04:10 to load 1585 files)\n",
    "<font color=\"green\">Success! with parallelization </font> (over 32 logical processors on \\\\inpdenagrogu), the rate has improved to (00:19:40 to load 1585 files) or *1.34 seconds per file* <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9367a8-c0df-44d9-9aa2-4815573ff160",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = dt.datetime.now() # we'd like to keep track of the overall processing time required\n",
    "AIS_gdf = Ais(list(iyore_subset)) # we load the AIS record in parallel\n",
    "end_time = dt.datetime.now()\n",
    "\n",
    "print(f\"Processed {len(list(iyore_subset))} AIS files in {end_time - start_time}, {((end_time - start_time)/len(list(iyore_subset))).total_seconds():.2f} seconds/file\")\n",
    "\n",
    "# bay_AIS = AIS_gdf.cx[-137.134247:-135.747100, 58.426598: 59.105969].copy() # a rectangular bounding box containing Glacier Bay proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf7a31-53b7-416e-8f34-b535ca753ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_AIS = AIS_gdf.cx[-137.134247:-135.747100, 58.426598: 59.105969].copy() # a rectangular bounding box containing Glacier Bay proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c94e70-2b65-49a6-b005-56114dcb3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_AIS.loc[bay_AIS[\"MMSI\"] == 367365630, :].plot(color=\"k\", markersize=1, alpha=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976966a8-4f3c-44c7-ba8e-bd66947ef24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# events = pd.Series()\n",
    "# for mmsi_, pts in bay_AIS.groupby('MMSI'):\n",
    "#     events[mmsi_] = len(pts['event_id'].unique())\n",
    "\n",
    "events.sort_values(ascending=False).head(200).to_csv(r\"C:\\Users\\DBetchkal\\Desktop\\common vessels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9f4ba1-41f4-4126-9f43-c6d1e908a172",
   "metadata": {},
   "source": [
    "### Step 3: find vessels passing nearby certain potential array locations\n",
    "if this cell throws an error, try running it a second time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6695cf-fb83-4e1b-862c-7fce917ad2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.io.file.fiona.drvsupport.supported_drivers['LIBKML'] = 'rw'\n",
    "gdf = gpd.read_file(r\"T:\\ResMgmt\\WAGS\\Sound\\ArrayScoping2025.kmz\", driver=\"LIBKML\")\n",
    "gdf.boundary.plot()\n",
    "plt.show()\n",
    "\n",
    "# gdf_out = gdf.to_crs(\"EPSG:4326\")\n",
    "# gdf_out.to_file(r\"T:\\ResMgmt\\WAGS\\Sound\\GLBA Array Scoping 2025.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91948c0e-e7c1-47d0-9cd1-a59517b049cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"North Marble W\", \"North Marble E\", \"Willoughby NW\", \"Willoughby SE\"]\n",
    "out = gpd.GeoDataFrame([])\n",
    "\n",
    "for array_bound, name in zip(gdf.geometry.explode(index_parts=True)[0], names):\n",
    "\n",
    "    array_pot=gpd.GeoSeries(array_bound, crs=\"EPSG:4326\")\n",
    "    array_pot_ea = array_pot.to_crs(\"EPSG:26908\")\n",
    "    sample_area = (array_pot_ea.area[0]/1e6) # km^2\n",
    "    \n",
    "    print(f\"Now computing intersection with {name} array.\")\n",
    "    AIS_clipped = gpd.clip(gdf=AIS_gdf, mask=array_pot)\n",
    "    AIS_clipped = AIS_clipped.sort_values('heading') # we want it organized by heading\n",
    "    AIS_ = AIS_clipped.loc[((AIS_clipped[\"TIME\"].dt.month >= 5)&(AIS_clipped[\"TIME\"].dt.month < 10)), :] # select only summertime events\n",
    "    AIS_.to_file(os.path.join(r\"C:\\Users\\DBetchkal\\Desktop\", name+\" clipped points.geojson\"), driver='GeoJSON')\n",
    "    print(f\"\\tClipped to an area of {sample_area:.1f} km^2. Visualizing...\")\n",
    "\n",
    "    fig,ax = plt.subplots(1,1, figsize=(8,7))\n",
    "    array_pot.to_crs(\"EPSG:26908\").boundary.plot(ax=ax, color=\"k\", ls=\"--\")\n",
    "    AIS_.to_crs(\"EPSG:26908\").plot('heading', cmap=cmocean.cm.phase, markersize=1, alpha=0.06, ax=ax, \n",
    "                                   legend=True, legend_kwds={\"shrink\":.5, \"ticks\":[0,90,180,270,359], \"label\":\"Heading (Â°)\", \"aspect\":10})\n",
    "    ax.set_title(f\"{name}:  {len(AIS_.MMSI.unique()) :.0f} unique vessels by MMSI\", loc=\"left\", y=1.03)\n",
    "    ax.set_xlabel(\"Easting (m)\", labelpad=20)\n",
    "    ax.set_ylabel(\"Northing (m)\", labelpad=20)\n",
    "    ax.ticklabel_format(useOffset=False, style='plain')\n",
    "    plt.savefig(os.path.join(r\"C:\\Users\\DBetchkal\\Desktop\", name+\" array scoping map.png\"), dpi=300, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\tPoints have been selected.\")\n",
    "\n",
    "    S = stats.entropy(AIS_.MMSI)\n",
    "    out.loc[name, \"unique_MMSI\"] = len(AIS_[\"MMSI\"].unique())\n",
    "    out.loc[name, \"S\"] = S # entropy, in bits\n",
    "    \n",
    "    print(f\"\\tVessel entropy = {S:.1f} bits.\")\n",
    "    \n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geosound",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
